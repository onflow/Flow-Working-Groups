# Meeting 02 - Core Protocol Working Group

**Date**: April 24, 2024 (Thursday), 10-11am PT (17-18:00 UTC)

## Agenda
1. [**Upcoming EVM Gas, Computation-Limit & Transaction-Fees updates**](https://forum.flow.com/t/proposing-transaction-fee-changes-and-flow-evm-gas-charges-for-flow-crescendo-launch/5817) (from governance working group) [15 mins]
2. Update on **Tuning Flow's Main Consensus**: finalized timing parameters, projected Time-To-Finality speedup, mainnet deployment steps, side-note on execution [15 mins]
3. **HCU-style upgrades for all node roles**: from of sub-working group [15 mins]
4. 

Anyone can **add agenda topics** by creating a pull request on this file and extending the list above.


**Attendees:**

## Meeting Content

### 1. [Upcoming EVM Gas, Computation-Limit & Transaction-Fees updates](https://forum.flow.com/t/proposing-transaction-fee-changes-and-flow-evm-gas-charges-for-flow-crescendo-launch/5817)

:microscope: Kshitij

### 2. Tuning Flow's Main Consensus

Core Protocol Sub-Working-Group met on April 4, 2024 [[video recoding]](https://drive.google.com/file/d/1kQBsju8iSn7k0hfsbxvJtkGu0eQP6sF6/view?usp=drive_link)

#### Key Results / Executive Summary

<ins>[Proposed parameter setting](https://www.notion.so/flowfoundation/Cruise-Control-headroom-for-speedups-46dc17e07ae14462b03341e4432a907d?pvs=4#67bc31aef34d4be6979a439b74a301c8) (notion)</ins>
* set block target average block time $\tau_0 = 800ms$ 
   * corresponding to 756,000 views for every Epoch (1 week)
   * this is the setpoint for Flow's Cruise-Control sub-system [[1](https://github.com/onflow/flow-go/blob/master/consensus/hotstuff/cruisectl/README.md),[2](https://www.notion.so/flowfoundation/Cruise-Control-Automated-Block-Rate-Epoch-Timing-Design-c9e94982ee2e49a9ac73b34039c424d1?pvs=4)], which controls the block time on the happy path for reliable epoch switchover every wednesday 19:00 UTC
* set `hotstuff-min-timeout` = 919ms 
   * minimal wait time for nodes, before they start to engage in timeout protocol for consensus round (dynamically increases when rounds fail successively)
   * this is the setpoint for Flow's PaceMaker, which limits impact of offline nodes and timing attacks

_Attention:_ [multiple details need to be correctly configured](https://www.notion.so/flowfoundation/Cruise-Control-headroom-for-speedups-46dc17e07ae14462b03341e4432a907d?pvs=4#4d398f482125429f9460bccdceebdc47) when bootstrapping Mainnet at the Crescendo upgrade
* will "practise" deployment at Testnet Spork (tentative date: May 22, 2024) 

<ins>Projected Time-To-Finality Speedup</ins>

* _Consensus's_ Time To Finality
  * duration from the time a block proposal $B$ is broadcast to $B$'s finalization

  > _HotStuff's_ Time To Finality is projected to improve by 36%


* _Transactions's_ **Time To Finality [TTF]**

   * measured from the time the client sends / Access Node receives a transaction $T$ to $T$ being in a finalized block
   * For the overall protocol, the Time To Finality of _transactions_ is the relevant metric.
     To avoid missleading impressions, **Flow's Time To Finality [TTF]** is the short form of _Transactions's_ Time To Finality.
     While Flow's convention follows good scientific conduct, it is unfortunately not common industry practise.
     Frequently, protocols use their _Consensus's_ Time To Finality for marketing.
     From a user's perspective, their Time To Finality is often much longer (e.g. factors of 2× to 5×), because in many newer protocols 
     transactions go through steps such as validation, batching, mempool-synchronization, etc prior to consensus.

  > Flow's **Time To Finality [TTF]** is projected to **improve by 15% - 18%**

<ins>Byzantine Resilience</ins>

Estimates:
* We operate with these timing setting for consensus committee sizes between 88 - 100 nodes.
* We can reach the targeted epoch switchover time with up to 15% consensus participants being offline (some level of uncertainty), with more conservative estimates around 10%.  

<ins>Side-note on execution</ins>

* properly pipelining collection execution and proof generation for verification is important 
* we want to minimize the overhead of a block containing few collections, each with only a few transaction

### 3. HCU-style upgrades for all node roles

Core Protocol Sub-Working-Group met on April 5, 2024:
- Flow forum post: [Protocol Version Upgrade Mechanisms Discussion](https://forum.flow.com/t/protocol-version-upgrade-mechanisms-discussion/5717)
- Meeting Notes: [[Brainstorming] HCU-style upgrades for all node roles](https://www.notion.so/flowfoundation/Brainstorming-HCU-style-upgrades-for-all-node-roles-b6b0ab084075432782cd0407b73479c7?pvs=4) (notion)
- Meeting Video: [Core Protocol Sub-Working Group: HCU-Style Upgrades for Nodes & Execution Nodes](https://drive.google.com/file/d/1fpQPiiM-K4_hEORZIV9fIdOqEW17Q4O5/view?usp=drive_link)
- Meeting Transcript: [2024-05-04_sub_working-group_transcript.md](./2024-05-04_sub_working-group_transcript.md)

#### Key Results / Executive Summary

* The Dynamic Protocol State provides primitives for
  * informing all nodes when a new protocol version takes effect and triggering restarts if required
  * adding custom logic (event-triggered) to run upgrade-specific logic
  * enforcing protocol consistency across all nodes (also handling late nodes catching up)
  * nodes, whose software doesn't support the specified protocol version, detect that they don't "speak" the required protocol version and will not participate (vs. participating on a best-effort basis, possibly disrupting the network, violating protocol rules and getting slashed)

* Using the Dynamic Protocol State for protocol upgrades is _still exploratory_: 
  * with the Dynamic Protocol State (reaching Mainnet with Crescendo Upgrade :sparkles: - tentatively end of June 2024) we have a mature proof-of-concept implementation
  * but need a lot more experience working with the available mechanisms before we can start to design more generally applicable upgrade frameworks.      
  > [Technical summary from the sub-working group]
  > The mechanism under discussion is limited to coordinating _when_ the upgrade will happen, not the nature of that upgrade. For example, the new software release could understand two protocol versions and be able to switch between them on the fly, _or_ the new version could require that the old software shuts down, triggers a data upgrade process, and then a new software release is launched on the new data. But in either case, this mechanism will ensure that there is an unambiguous “time” (i.e. view) when the new protocol version is active and the old protocol version is obsolete.


* sub-working-group discussed impact of the primitives for protocol upgrades that the Dynamic Protocol State provides:
  * broadly useful
  * precise impact hard to quantity (depends on nature of upgrades that we have to ship next year and how urgent they are)
  > [Quote Dete] I don't know but it's inconceivable to me that this is not useful and valuable in the next 12 months to release software that at least we care about and more easily

## Key Outcomes

### 1. abc
### 2. xyz

### Links and further reading
- Meeting Transcript: [<file name>](./yyyy-mm-dd_transcript.md)
- Video Recoding: [<file name>](https://drive.google.com/drive/u/0/folders/1WMECJSa-ySSNvcuPFhFn8d7m8dAXL6b7)
