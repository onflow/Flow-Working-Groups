# Meeting 12 - Core Protocol Working Group
**Date**: April 24, 2025 (Thursday), 10-11am PT (17-18:00 UTC)

## Transcript

**Alex Hentschel**:
We are going to start with a malleability topic, guaranteeing immutable data structures in our Golang codebase. That's Gordon's first topic. And then we can spend the rest of the time on the state and event proofs. I assume that there could be some relatively deep technical questions. So my thought is that we can cover the higher level here and then delegate some of the more nitty-gritty technical details and discussions to a sub-working group where we can meet if that's possible. Sorry, if that's necessary, or maybe we get to all the questions here, and then we do the rest sort of in written on the document. All right, that's it. Thank you. Jordan.

**Jordan Schalm**:
Cool. Yeah, so I'm going to talk through a new extension essentially to the Malleability workstream. Feel free to jump in with questions at any point. If you're not familiar with Malleability, essentially it's just going through all of our existing data structures, and for those which have a canonical hash method defined, which is usually the ID function, making sure that that hash is collision resistant, makes the mental model a lot easier to work with, and also is a big risk for attack vectors. So that was kind of the initial scope of malleability, and we want to extend it to kind of enforce that close to that same set of models are immutable in almost all circumstances. So there's kind of two parts to that. One of them is going through our code base and removing instances where we're either constructing models in a whole bunch of different ways or mutating an instance of the model that's already constructed. And the goals of that is basically, again, to kind of simplify model. So we want to minimize the scope or the service area where super important for data structures are allowed to be constructed or mutated. Almost all the time they should be immutable and we want to enforce that more strongly. And then some extensions to that we want to be able to explicitly differentiate between a model and a model under construction. At the moment there are several places and the code the most notable is probably the block header where we kind of blur those boundaries and it kind of results in a mess that's quite difficult to reason about correctness. And then the other downstream benefit is that if we are able to basically guarantee or have a very strong certainty that a certain model is going to be immutable during the course of its lifetime then we can things like cache its ID method. So right now we have a whole bunch of different places in the code where we're using a bunch of different strategies to avoid caching and encoding data structures in a bunch of different ways, and we want to kind of consolidate that. So you should be able to take a data structure, a call ID, and know that we're only ever going to encode it and hash it once after construction. Cool. Yeah, so there's a little diagram here. This is kind of, in my mind, the three major sources of where data structures are, well, data structures in the sense of our core data models like blocks and execution receipts and stuff like that, where those are instantiated in the context of our code base, where to get them from the network, which is an untrusted source, we have to get database, which is, broadly speaking, a trusted source, or we're constructing them ourselves. And the rest of this is, this presentation is going to talk about the tool that we built to kind of help us with this journey of making our data structures immutable with Solentor. And in that context, we're primarily talking about this middle one, where we're constructing data structures within the business logic. It's mostly not going to worry itself about these unmartialing circumstances. Any questions or comments so far? Yeah, so over the course of the last week or so we've made this winter which is called StructWrite, and this is a tool that gets integrated into our existing linter system, Golang-CI-Lint. This has already been implemented and merged, and it's already part of CI, but it will only do anything if you opt in a type for the linter to look at that specific type. So at the moment, it's running, but there are no types that have been opted in to this additional mutation protection. And basically what it does is it detects any kind of disallowed either mutation of an existing variable or a kind of construction that we don't allow and then it reports an error and then that will cause CI to fail so you won't be able to merge your PR to master. Yeah, it's type granular so when we get to the point of adding a type and going through and removing any instances of unsafe or undesirable mutation we'll opt in that type that make those changes. The way you opt in a type at the moment is by adding a directive comment to the type definition. So as an example, right here, if you add this directive comment, that will tell the linter that you want to opt in this type. And it's disabled for test files so that you don't all hate me And yeah, here's a brief overview of the rules. So here's an example type that we want to opt in to mutation protection called immutable type. We've added this tag to tell the winter that we want to protect it. So it has one field, which is an integer. And basically, you are allowed to construct an empty instance of that type, so like variable declarations without actually providing data are allowed, or you are allowed to construct it using kind of a blessed constructor function. That's the simplest example. Alternatively, for more complicated things like a header, we would likely use a separate type to represent the header under construction and use sort of a a builder method, similar to the component manager builder, if you're familiar with that, use that kind of pattern. But in any case, the goal is that all of the logic where we're sort of constructing and codifying what represents a valid instance of the type is sort of pushed into this smaller space. So it's pushed into these constructor functions. And then obviously you can't, you can't mutate a field, So this would be flagged by the linter. This would also be flagged by the linter if we're kind of going through multiple embedded structures, so it does deal with embedding. And then similarly, if we try to construct an instance without using constructor, that's going to get flagged as well. If you just want to declare an empty variable, that's allowed, and you're also allowed to assigned a variable. There's a link to the code further up right here in this. Yeah, you can go ahead.

**Dieter Shirley**: 
Yeah, so if I create an empty one, then it's just an empty one, like I can't assign to the field still?

**Jordan Schalm**:
You cannot assign to the fields, yeah. Is that ever useful? Sorry. Yeah, it's a good question. This was like the main thing that we were going back and forth on and on how we should handle this case. There are kind of two main things that I could think of for why we want to do this. The first one is basically the left and the right hand side of this diagram when we're un-marshaling stuff. The way that works in Go is you declare an empty variable and then you pass it into the un-marshal function and then it uses reflection. To fill in all the fields. That's the big one. And then the other thing is if you're just, for example, like if you're going to construct a, let's see, I want to construct an instance of immutable type depending on some conditional like this. You might want to declare it out of time.

**Dieter Shirley**: 
And that assignment is fine, because you're assigning an existing one rather than mutating. Like you're replacing it wholesale rather than modifying it. Yeah, that makes sense.

**Bastian**:
So is it like tracking that x was sort of like an empty, uninitialized value, and then the assignment is correct? Because the exact problem top just like not allowed like x is immutable type so the reason this is not allowed is because where it's because of this part it's not because of the assignment oh so this construction is not allowed because we want to say like you're going to define the way that you construct a type once ideally and you just saw the constructor function not really constructed I thought it was the assignment like the right like because the line before is like assigning And I felt like, oh, this kind of assignment where you're overriding it is also not allowed.

**Jordan Schalm**:
But it was about the construction, not about the assignment. Yeah. So like, for example, this would be allowed. Reassigning it, but going through the constructor again. That would be allowed. Makes sense. Cool.

**Dieter Shirley**: 
If you want to make that more clear, Jordan, it should be why and then colon equals, I think that that line then would be obvious what's happening. Or just make it a colon equals of the x, since you're using x a bunch of places. Because the problem isn't that you're declaring it that way, it's that you're instantiating it that way.

**Jordan Schalm**:
So if you put a colon equals after the x.

**Jordan Schalm**:
Yeah, yeah, exactly. Yeah, good point. Cool, yeah, so this is like a very brief overview of most of how the rules work. There are many more examples and test cases, like I mentioned, in the code. Or feel free to ask me how it works. Or if you think it should work differently, feel free to raise that as well. Yeah. There are a few claps.

**Tarak Ben**: Youssef 
Yeah, Tarak? Yeah, it's simple to define these rules. You showed us how we should not you be using this or how the linter, what linter will catch what?

**Jordan Schalm**:
Is it easy to configure this? Is it easy to extend what the linter will catch? It kind of depends what you want to do.

**Tarak Ben**: Youssef 
Okay. Heavy coding or just like defining things in JSON settings? How does it work?

**Jordan Schalm**:
So it's like the way the linter API works is you get the go or the linting tool will run it'll generate an AST of the code base and then it will kind of hand you the AST and then you iterate through the AST abstract syntax tree. So you do need to actually write a function which will go through the AST and do what you want it to do. So it's not, it's not, it's not like there's a set of rules. And then in config, you say, Oh, I want this rule and not that rule, you actually have to implement them. Existing like the stuff that's already there, it's not super complicated. It's maybe, I don't know, maybe like 100 lines of code and lines of comments. Some things which which seem easy could potentially be very hard and vice versa. It depends. OK, thanks.

**Unidentified Speaker**: 
Cool.

**Bastian**:
And then, yeah, so those are the rules.

**Jordan Schalm**:
There are a few classes of kinds of patterns where you can mutate a data structure, and it will not get caught by the lender. So the first one, and this is one that we, for the most part, want because this is very useful to use this all over the place, which is if you are un-marshalling from the database or from a message sent over the network, the way you do that is you declare an empty instance and you pass in a pointer and then the un-marshalling function will use reflection to match up fields in the encoding for your data structure and fill them in. That is a new technique but that is allowed. So that's the first one. The other one is any use of unsafe.pointer, which disables a lot of the safety features of Go, and it's very hard to reason about. I didn't actually look too closely into what you can do with this, but I'm pretty sure you can break the mutation protections if you're using that. We don't really use that in our code for the most part.

**Dieter Shirley**: 
Can I suggest when you get, if you ever are iterating on this that maybe you, well, I don't know if you can know when unsafe pointer is used with one of these data structures or not, like just basically saying, don't use unsafe dot pointer when one of these things are involved. Because obviously, you can't tell if it's being mutated once it's turned into a pointer. But if you can catch the most obvious way to turn it into a pointer and restrict that, then that might keep from accidentally foot gunning.

**Jordan Schalm**:
Yeah, that's a very fair point. Yeah, that's very fair. And we could also potentially do something similar with number one, but we just reuse that pattern way way, way, way more time to use a safe dot pointer. And then the last one is this sort of contrived example, but another way that you can get around the mutation protection is by basically reassigning to a variable, which we explicitly allow after you've shared a reference to that variable. So I'm not sure how easy it will be to parse this code in a short time, but basically we create y, so s has a field of type y and then y has a field itself. So we create an instance of y, totally fine. We create an instance of s, but we pass in a reference to y. So at this point, x dot y dot b equals one, which is from here, and it's generated y. And then if we just reassign this variable, since it has a shared reference, that is going implicitly mutate their shared reference. So this is kind of a one major gotcha that I've been able to find, but this is not going to get caught.

**Dieter Shirley**: 
Do we even use pointers inside the immutable structs?

**Alex Hentschel**:
Yes, we do. Massively, because that's kind of, it's very memory efficient, right? Because otherwise, by engulfing if you use value types, it always copies everything on assignment, right? So if you have a nested data structure like a block, which contains, for instance, a quorum certificate, right? And we don't use pointers. And each time you would sort of assign the header to a variable, it would copy the quorum certificate or all the data structures contained in the header, right? Trying to avoid that. Maybe an execution receipt is a better example. For instance, the execution receipt has a pointer to the execution result. So that essentially when you create an object of execution receipt and you kind of hand that to different functions, it doesn't each time copy the content of the execution result the receipt is kind of certifying.

**Dieter Shirley**: 
You know what I mean? I totally know what you mean. I just don't know why we don't mandate using pointer for the outer object rather than using pointer on the inner object.

**Dieter Shirley**: 
Well, because when we started the code base, we weren't worried about that.

**Dieter Shirley**: 
Yeah, I mean, it's fair enough. And but it, you know, having those things as pointers is its own cost. And so if maybe over the long term, we can take a look at that. And one tool that we could look at that is for maybe this linter in the long run to basically say, well, look, if you have a pointer inside of one of these immutable things, that's no bueno. But yeah, I totally understand that if there's a bunch of existing code, we don't want to just go in and fix it. But I will say that you're probably There could be some performance hiding underneath that that might be a big deal if we get to that point.

**Tim**: 
I'd like to say there's definitely one case in which I don't think we can avoid this, which is lists or slices. Which are, when we have a variable number of something in a data structure, we use a slice, we can't store those directly as part of the parent data structure because we don't know how many there are beforehand. So, we have to have the slice is just a pointer as well as the length, right? So, in that case, I'm not sure if we can avoid this issue? No, and certainly you couldn't in that case.

**Dieter Shirley**: 
I mean, you know, I suppose you could allocate the maximum number, but that sounds pretty awful. But certainly in the examples that Alex gave, you could get rid of those pointers. And I think that you'd find that, you know, if those data structures were used a lot that could actually provide an efficiency gain. But again, it's kind of all academic, because we have existing code. And we're not going to change it one way or another for any of the reasons we just talked about.

**Jordan Schalm**:
We're going to be using this as we kind of opt-in types to the system and remove instances of unsafe mutation. If you're adding a type, obviously feel free to use this new tool. And if you have feedback, feel free to let me know or post in the workgroup channel.

**Alex Hentschel**:
Yeah, and the guideline is generally everything which goes over the wire from one node to the other other, of course, we want to be able to attribute illegal data mutations, right? If you have a Byzantine node, that shouldn't be able to mutate the data. So everything that goes over the wire has to have a cryptographic hash and has to be signed. That's all there already, right? But that also means that when you, inside of a node, are processing those data structures, you shouldn't be mutating that, right? And so everything which goes over the wire should be monitored by this linter that those data structures are not mutated accidentally. And yeah, because only then we can really sort of guarantee that we actually catch, well, that we don't mutate things and that therefore it's, for instance, safe to cache the IDs, but also we aren't tampering the honest nodes aren't accidentally tampering with the data, you know, because we've overlooked some sort of code paths.

**Jordan Schalm**:
That's what we're going to explore.

**Bastian**:
So everything that flows over the wire needs to be protected by the sender. I saw this in the Flow Engineering Slack channel when you posted, and I was like, oh, that's great. We do something similar on the cadence side. Parts of this we already have there.

**Jordan Schalm**:
Oh, really?

**Bastian**:
Yeah. So the constructor call for a struct. In Go you can enforce that a struct is constructed through some kind of constructor function that enforces some kind of correct instantiation. So we kind of have that already there. Another thing we have is a linter for ranging over maps, because that's not a deterministic goal. And that's like, sort of, you know, you can't have another deterministic code. And when I saw it, I was like, oh, this is also great. Because like in a lot of places, we probably want to enforce that as well there. So maybe we can move this into another repo. And then we could share it. It's sort of more like infrastructure tooling repo or something, because of the, you know, FLOGO depends on cadence, so we can't really import it there either. It's kind of like cyclic dependency, but it would be really cool to use that as well.

**Jordan Schalm**:
Yeah.

**Alex Hentschel**:
Could you do in one of the future working group presentations, you or Supuna, anyone from your team, talk about your approach to maps, because we have that problem too, that from a business logic perspective, we want to represent something as a map, right? But if I send you a block and it has a map inside, and you then forward the block to Jordan, right? It's still signed by me. So you need to be able to reproduce the order for the map, and then not just list them in different order, because otherwise cryptographic would be different. And so by now, we have done that by hand. It's not there in many places, so it's doable by hand. When you iterate over it, do you want to update something?

**Bastian**:
Well, when you have something like this in ordering, yeah, compute the ID.

**Alex Hentschel**:
Then it's like, oh, OK, it's a map. I know I first have this field as a map, so I have to first sort of linearize it in a specific order to a slice, and then I can actually slice. So we can code that by hand to sort of enforce an explicit ordering. And also when the mask goes over the wire, I think. But if you have some automated tuning there, then I think no.

**Bastian**:
No, it's like you just have a wordless data structure that keeps it ordered all the time.

**Alex Hentschel**:
All right.

**Bastian**:
And it doesn't work like a map, or is it? Well, it keeps a map under the hood plus a slice. Oh, OK. And then it's fixed and ordered when we need an ordered. You use a master's assets, like it doesn't matter. And you never iterate, we only check for containment. So like, it doesn't mean these are the two tools that we have. But really often, like you, you want to keep it ordered, because like, you keep on iterating over it. So like, just keep it ordered. And then in cases where it's just like, you have to do one thing for each element in the map, where it's X as a set, then still, you need to do it in a you know, potentially, like, not in the wrong way, where, like, the ordering is, you know, you have some side effects. So that we have that limiter in place. And that works really well. And, like, how many times we caught ourselves, like, oh, yeah, yeah, you know, it's just quickly do a full range loop. And, of course, it would have been bad. Yeah. But it's also in the Cadence repo at the moment, so you can't really use it for Flow Go.

**Alex Hentschel**:
Maybe we can just put it in the shared repo somewhere.

**Dieter Shirley**: 
Yeah. Yeah, it's slightly different, because Cadence has to iterate over the map the same order every time, whereas I think that protocol, you only care that it's serialized in the same order every time. So if I have three keys, A, B, and C, if one instance of cadence is iterating over that map and it goes ABC, every other instance of cadence has to go ABC, right? Because any code below that could be part of the Spock proof. And so they have to do the same order. Whereas for the protocol, you probably don't have that restriction. Although it might make some of your stuff easier being able to trust that everyone's going to iterate the map in the same order.

**Bastian**:
Yeah, so for us it's because of the execution result, we have to, you know, but like FVM also like there's so many things and should do them deterministically, like, you know, it's kind of like the function always flip cadence doesn't know how to do certain things. This is like, please create an account for me, like, you know, FVM, like, we don't know how to do that, like, we have to do it and then they have to still enforce those guarantees that we have as well, like, maybe for networking and for Are you ready to transition to the next topic?

**Unidentified Speaker**: 
Cool.

**Alex Hentschel**:
Sorry, I think the next topic is also going to have a lot of questions.

**Dieter Shirley**: 
I do think, just very briefly, I do think it might be worth picking up on what Tara said earlier and what you said, Alex, is that it might be worth having another session somewhere where we talk about these kinds of tools, and we integrate maybe what Cadence is doing with what Jordan is doing, and then we explain how it works to Tarak, because it sounds like Tarak was thinking that there's some linting stuff that maybe he'd like in his code. So it might make sense for there to be an internal tooling presentation or something at some point about this approach and how other people might be able to leverage it. I love this stuff, and I'm really glad to see that you did this, Jordan, and great. I'm not surprised, but very pleased to hear that Caden's team is doing something similar. And so I think if more people can think about how to leverage this tool, especially if it's half built already for their use cases, or entirely built for your use case, but for their use cases, probably leverage the in front of that would be awesome.

**Bastian**:
Maybe even like ask like, Tarak, you sound like get some ideas of like things you wanted to maybe have checked or not checked.

**Bastian**:
So, you know, how to achieve that maybe there's some other things you want to guarantee.

**Jordan Schalm**:
I don't know what that would be.

**Alex Hentschel**:
So let's transition to the second topic, state and event proofs. I think it's maybe useful to first start with explaining the scenario for which we sort of envision this to be used, and maybe for me to share my opinion why I think in the long term that's an important feature for flow. We envision Flow to be a consumer blockchain, right? So it has no sort of technical entrance hurdles, both for people to use it, so for clients as well as developers. And so, and a very common approach in blockchain nowadays is that if you want to be sure about the data, the blockchain data you're getting, then you either have to trust someone, so you trust the service provider who then gives you the data, or you have to run a node which more or less synchronizes every block and validates it to some extent. And if you think, for instance, of something like Robinhood, you know, that sort of stock trading app on your phone, you know, I don't know. I think of myself coming out of the woods, you know, after a three-day hiking trip wanting to check on my tokens or something in my account, but I don't want to trust anyone. And I also don't want to wait for my telephone for five hours to sync all the blocks for the last three days I've been in the woods. So how can my phone be sure without needing to rely on a trusted service provider? Because what's the point? It's a little bit of an extreme. Standpoint, but you can ask the question, well, what's the point of a blockchain? If for a normal person to interact with that blockchain, they anyway have to trust somebody at the boundary of the blockchain to give them the correct data, then why do we build the blockchain all inside and don't just anyway rely on a centralized service provider, like a regular bank or something like that? So that's kind of the mindset here, that we want to be able to give external clients data about the chain without those clients needing to trust the service provider and the clients not having to synchronize all the block headers. And then the question as well, how do we do that? And how do we deal with the fact that every, that potentially, if you don't trust the node, you have to consider that the node you're talking to you're getting the data from, like an access node, that that could be potentially Byzantine. And then the intuitive answer is, well, then the access node has to give my client proof. So I'm asking for a certain set of data. I'm getting the data, and I'm getting proof from the access node that this is the reason why the data I'm giving you is correct. I think it's relatively straightforward to talk about proofs when you assume that the recipient of those proofs, let's call them the verifier, which is not to be confused with a verification node, it's just the external client. If that external client has all the blocks and all the headers, then it's more straightforward. More of the sort of the conventional settings, right? Then we can talk about Merkle state groups and things like that as of a certain block. But how does that work if I wanna think about a scenario where the external client does not have all the block headers? And so this is kind of what this entire conversation is about. And for instance, one application that is where Deniz is already originally started is, so he wants to verify on one chain that something has happened on a different chain. And of course, for on-chain logic, it's kind of not really practical to synchronize and verify all block headers from the different chain. So there we rely a lot more on various sparse data that the client has. The client in this case would be the on-chain logic and has various sparse data and the data provider, so the prover, will have to do a little bit more work to convince the on-chain code or the live client running on a cell phone to convince that client that the data it's being given is correct. Okay, so I'll pause here and see if there are questions about the setup. Does that make sense? Is there anything you would All right, no questions, good, good. So let's talk a little bit about preliminaries. And I think that helps us to also kind of understand better where we're going with it later. So initially, so let's say I want to install a trustless wallet on my phone. In order to install that wallet, I need to get the wallet software somewhere. If you talk to a Web3 maximalist, they will realistically or will a lot of times suggest something which realistically boils down to, well, you read the protocol specification of the protocol and then you implement your own live client or your own client because then you're sure that it's all honest. And then you synchronize all the blocks to get the latest date, right? So, but for a normal person and a normal business, this is completely intractable. Realistically, how that boils down to is the Flow Foundation will provide the light client software, right? And somebody wanting to use or run a light client will say, okay, at least the software provider is reasonably trustworthy to me, right? So they get the software, the light client, and software from a reasonably trusted source. And hopefully it's open source that people in the security community can review it, but realistically you will get your software from a trusted source. And at that point, I think it's also very reasonable to say, okay, so from the person you're getting the software or me, I mean, at that point it's anyway, me as a human being involved, right? I need to go to the Apple app store and install the software on my phone. And so I need to know, am I installing that software from a trusted source? At that point, I'm as a human anyway involved, and I'm telling the computer, yes, you can trust the software. And at that point, I think it's very reasonable to also say, okay, we're providing some sort of, I call this the root of trust, which is more or less some reasonably recent but historical blockchain data, which says this is the correct blockchain you're talking to. If you want to talk to Flow, the Flow blockchain, for instance, has this chain ID, and those are the 87 consensus nodes that are allowed to sign blocks. This is your public key, public keys, public staking keys, and this information is from epoch 47, ranging from views, I don't know, 10 million to 10 million, $311 million, something like that. So this is the data that a human would also kind of acquire from a trusted source and provide alongside with the installer for the software, for the Lifeline software, provide that to the phone. So that root of trust. And I think that is a very important aspect to first say, OK, some human is providing this. And that is the only data we're trusting, or the only point where trust, I guess, is required. And from every point onwards, the live client will make sure, will guarantee that if that original route of trust is honest, the live client will guarantee, essentially similarly to an inductive proof, that all the subsequent data it downloads and it relies on subsequently is guaranteed to be honest by the root of trust. And so that's kind of the flow I would like to go through. So specifically what data would the live client get and what would that data need to mean and what would need to be checked for the live client to convince itself that all the data is honest. Okay. How does that sound?

**Alex Hentschel**:
And I think a good example here is event proofs. So let's assume somebody submitted a transaction, and that transaction submitted, or when it executed, it generated a specific event. And then the live client will say, I want to know that this event happened. So if you can prove to me that this event happened, for instance, a deposit of tokens has happened into my account, then I'll do certain things. And so I think that's a good example. And so, yeah. And so I think first, the first step is to say, okay, the live client has information about some past epoch, which it can verify, and then the event happened potentially in the newer epoch. So that's kind of the first step where the live client needs to get the information about the latest epoch. This is kind of briefly summarized in the flow data ingress vision. You can look into there. Yeah, yeah. It's this write-up about smartphone-sized light clients. And so in a nutshell, what would happen is that when the light client knows the Epoch, knows the consensus committee for Epoch NR, for the R stands for truck to route, route of trucks. So that's what the light client has been initialized in. Within that epoch N, R, the consensus nodes of that epoch essentially sign information which says what the next committee is that will take over from them. So they say, we're gonna hand over, let's say from view 11 million, we will hand over the operation of the network to those other 80, seven consensus nodes, and here are their key. So the trust or the consensus committee, which is assumed to be honest in epoch NR, explicitly states which is what the consensus committee is for the next epoch. So an epoch is one week long. So here you can see how kind of the lifetime can go from one epoch to the next epoch and get for each epoch, get the view range, and get the public state keys of the consensus nodes and their node IDs. And that's relatively little data per week. And that's possible to get if we cannot, by the same mechanism, I guess that's what I'm trying to say. So the live clients can get the information about the next epoch by the same process as how it would get any data about what had happened in epoch NR. So it knows the epoch NR, and it can verify statements consensus nodes make in that epoch NR, including the statement that the consensus nodes from epoch NR are going to hand over their operation to the network to the next consensus committee. So that's all what the light client can verify. And so now we'll go a little bit more into the step where the light client knows the consensus nodes configuration for some epoch, and we want to prove that something happened within that same epoch where the live client already knows the consensus to MIDI off. Does that all make sense?

**Alex Hentschel**:
Yeah, so, okay. So let's walk a little bit through the steps in detail. So whenever we were saying the consensus nodes have made a statement, most of that means, most of that times, or most of that means that the consensus nodes have certified that they have checked and approved certain amount of information. FUDs, that's good enough because we're saying, While if the consensus committee is compromised, then anyway, the blockchain is true. And most blockchains make, or all blockchains make that assumption that the Byzantine threshold is below, or the Byzantine participation is below security threshold. So anything that the consensus committee essentially approves, we take as granted. And so, and when we say the consensus committee has certified something, typically what that means is that that information is in the finalized block, right? Because they also say something is certified in pending blocks, but it might be that those pending blocks are orphans, right? So we typically only want to rely on information that is in the finalized block. So the first thing the like client has to do, or the prover has to say, here's a section of blocks that's within the epoch you know, and I proved to you that the need newest block of that section of blocks here in the diagram, we see that block A, block B, up to block Y, that's all one connected sequence of blocks. And the Hoover says, and the first thing I'm going to prove to you is that the newest block of that sequence is finalized. By that, then you know that all the ancestor blocks of that in that sequence have also been finalized. So you know that we're only talking finalized information. And so here, we essentially only need to verify the Tworum certificate that the only BLS check or the only cryptographic signature check that we have to do is we have to verify this Tworum certificate. And this Tworum certificate essentially is an aggregated are the aggregated votes for the highest block from a supermajority of consensus nodes. And then there are some rules regarding views to convince the live client that block Y is actually certified. It's written up here, but I'd like to skip over that. Essentially, it's just the hot stuff finalization rules. The live client evaluates, says, okay, the highest block is certified by the consensus committee. I know their signatures, sorry, their keys. Therefore, I can verify that this is a valid signature over block Y. And then I apply the consensus rules, which are really straightforward. It's essentially just this, those lines. And then I can convince myself that block Y is finalized. Great. And then I can walk up the chain. I hash the parent block and verify that the hash of the parent block matches what the child block is stating as its parent, right? So I kind of verify that. I first verify that this highest block is finalized, and then I verify that this is actually a connected sequence of blocks. And then let's say I have the event we're caring about is in was emitted during computation of block A And we will see in one of the descendants of block A that there is a seal for the result of block A And again, the light clients knows that the seal is honest because it has verified that a finalized block Yeah, that is part of the finalized chain. So the consensus nodes have essentially applied all the verification effects, all the checks. So then the live client knows that the consensus committee considers this seal for block A as honest. And then it can look up what the execution result is that is being sealed in this block B, the execution result, right? And I mean, this would all need to be packaged accordingly by the prover. Node, right? The lifetime can go to the access node and say, hey, I don't know, what is the latest event of this type? And then the access node would say, OK, here I'm giving the long sequence of blocks. You can please verify that that sequence is finalized. Then in the fifth block, you will find the field. And that field, the execution result with the event you are looking for. And then the live client would get that, would also, they would verify that all the hashes matches. So essentially everything here is just only hashing, right? It hashes, it hashes the execution result, sees that the seal is actually for that execution result, then it gets the execution data, it hashes that, and compares it against to the hash commitment in the execution result, and then essentially unpacks that step by step. And in the end, it will be like, oh, okay, there is an event in here, right, which has been hashed into the execution data for the result. The result has been hashed into the seal, and the seal has been hashed into the block, and the block has been finalized. And then the live client knows just by inspecting this sequence here of blocks, right, without needing to know any of the sort of further history. It doesn't need to know anything here to the left. It only needs to know of those blocks. And then it knows that that event actually has happened. Yeah. So that's more or less the high-level overview. I would like to point out one thing, and then we'll maybe go into questions, is that here, we're proving that something exists. We don't. So far, I'm not concerned with proving that I give you everything according to a certain criteria, right? So as an access node, the access node can say, I prove to you that an event has happened in block eight, right? But that is not a proof that no other events of this type have happened in other blocks. So that is not a completeness proof. I guess that's all what I'm trying to say. We have a design for that. I think that is Rampton's Clover proposal, but that is a little bit fraudulent because that requires a different type of trial to make those proofs. But for now, we can already prove that things have happened. On-chain, we just can't prove that I'm telling the light client about all those things that have happened on-chain. But we can still prove that something has happened on-chain. Okay, that's the sort of level I want to go through in this meeting. All right, questions. Good. Go ahead, David.

**Dieter Shirley**: 
So I know we only have three minutes, so we probably won't be able to get entirely through this. But I want to make sure I understand. So let's say that I am a client, and I have a quorum certificate for Epoch 5. And I go to an access node, and I say, tell me about something that happened in block 100 in Epoch 12, okay? So the 100th block of Epoch 12. What's the minimum amount of data that the access node could give me for me to be cryptographically certain that what it's telling me is true, like to verify what he's telling me is true?

**Alex Hentschel**:
So you said you're in Epoch 5?

**Dieter Shirley**: 
I have a quorum certificate for Epoch 5 that I trust.

**Alex Hentschel**:
I would like to rework that. You know the consensus committee of Epoch 5, so the public keys. So you can verify any foreign certificate of Epoch 5. Sure. OK. Then the first thing the live client is to be like, well, OK, you want to go to Epoch 12? For that, you need to first go to Epoch and know the consensus committee for Epoch 6.

**Alex Hentschel**:
So it will first give you a finalization proof for some blocks of Epoch 5, that's maybe something like five blocks. It will give you five blocks, and those five blocks will contain, for instance, either service events that determine the consensus committee for Epoch 6, or it will contain a hash of the protocol state snapshot, which also specifies the next the next consensus committee or the consensus committee of epoch six. So essentially, a few blocks at the end of epoch five, it will prove to you that those blocks are finalized. And those blocks will say, we're going to be handing it over to this other consensus committee of epoch six.

**Dieter Shirley**: 
So you said five blocks in a row, why isn't it three blocks? In a row?
Because of finalization?

**Alex Hentschel**:
Well, maybe it should be three blocks.

**Jordan Schalm**:
Well, but you also need the finalization proof. All right, but I mean, like, you need the information for block A, and then whatever the finalization proof is.
Is that fair to say? 

**Alex Hentschel**:
Yeah, sure.

**Dieter Shirley**: 
And the finalization proof is the two blocks after A, right?

**Alex Hentschel**:
Well, it can be more, but typically it's two blocks. Exactly. So maybe it's three blocks. All right.

**Dieter Shirley**: 
I just didn't think it could be more than that.

**Alex Hentschel**:
We can talk about that offline. I mean, it can be more if blocks aren't immediately finalized. So it will be three or a few blocks.

**Dieter Shirley**: 
And then I do that for each epoch going up to 12. And now you've proven the consensus committee for epoch 12.

**Alex Hentschel**:
Exactly, and now you can verify anything that is certain, any block that is certified by that consensus committee of epoch 12, right, and so now the access node would pick out, would need to pick out, when you say I want to know about x happening, the access node would need to pick out in which blocks of epoch 12 x happens, and then give you those blocks and the finalization proof.

**Dieter Shirley**: 
Right, so it would be like So for the sake of argument, if I wanted something from block 100, as I said, you'd give me block 100, and then however many blocks after that are required to prove that that block was finalized. Exactly. So I would get three to five blocks for each epoch, including the most recent epoch, more or less.

**Unidentified Speaker**: 


**Alex Hentschel**:
Exactly.
Makes sense? Good. 
Maybe before we wrap up, Deniz, do you want to continue this conversation? I'm happy to. Tarak also had thoughts, I'm happy for us to kind of maybe meet with a smaller circle and go through details.

**Deniz Mert**: Edincik 
Oh yeah, it's like in my first look at the things, I was thinking it would be like a little bit much easier, but now I understand all the systems, like this QCs and how it can like blocks can orphan and stuff like that. So I just dived into it. Like I have an unfairly good understanding of it. I will try to make a like proof of concept for myself about like the thing I mentioned from like transferring some assets from testnet to the mainnet. But it's like It makes totally sense. Just I am thinking like if there can be some tricks we can evolve that like we can make it like with less data somehow or like less blocks. Yes, yes.

**Alex Hentschel**:
I think the biggest challenge is that our data structures are actually not optimized to kind of prove that something is part of the data structure. Typically, it's like you have to take everything, you have to take the entire execution results to know that, I don't know, one event of a few hundred bytes has happened, and you have to download megabytes. But that's only because this is very sparsely merkelized. We could optimize it the way how we merkelize those data structures, that you don't have to download the entire data structure, and you can just get a proof that only the event has happened instead of needing to get, for instance, all events. But I think that's a good topic for a follow-up discussion. Okay, then Tarak and I will keep extending this document with a few more things, and then we'll, I guess, keep the conversation on Discord for now. And if there's anything where it makes sense for us to hop on a meeting, then we'll do that. Thank you very much. Amazing. Thanks, Deniz, for joining. We're three minutes over time.

**Alex Hentschel**:
Let's wrap up.
